{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import wget\n",
    "import glob\n",
    "import wandb\n",
    "import shutil\n",
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision.datasets.utils import check_integrity, download_and_extract_archive, download_url, verify_str_arg\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import shutil\n",
    "from PIL import Image,ImageFile\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.testing as torch_testing\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "from dataclasses import dataclass\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import pprofile\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.multiprocessing.set_start_method('spawn')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Add this line if you don't want to use GPU\n",
    "device = torch.device('cpu')\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`segmentation mini-project` CNN architecture, training loop, perform hyperparameter search and evaluate the best segmentation module. Inspired by the [U-Net](https://arxiv.org/abs/1505.04597) architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in shape:  torch.Size([32, 32, 128, 128])  upsample with filter size  4 ; stride  2  -> out shape  torch.Size([32, 16, 258, 258])\n",
      "in shape:  torch.Size([32, 32, 128, 128])  upsample with filter size  4 ; stride  4  -> out shape  torch.Size([32, 16, 512, 512])\n",
      "in shape:  torch.Size([32, 32, 128, 128])  upsample with filter size  4 ; stride  8  -> out shape  torch.Size([32, 16, 1020, 1020])\n"
     ]
    }
   ],
   "source": [
    "def upsample_block(x, number_of_filters, filter_size, stride = 2):\n",
    "    output_channels = x.shape[1] // 2\n",
    "    x = torch.nn.ConvTranspose2d(in_channels=x.shape[1], out_channels=output_channels, kernel_size=filter_size, stride=stride)(x)\n",
    "    x = torch.nn.BatchNorm2d(x.shape[1])(x)\n",
    "    x = torch.nn.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "in_layer = torch.rand((32, 32, 128, 128))\n",
    "\n",
    "filter_size = 4\n",
    "number_of_filters = 16\n",
    "\n",
    "expected_shapes = [\n",
    "    (32, 16, 258, 258),\n",
    "    (32, 16, 512, 512),\n",
    "    (32, 16, 1020, 1020)\n",
    "]\n",
    "strides = [2, 4, 8]\n",
    "\n",
    "for expected_shape, stride in zip(expected_shapes, strides):\n",
    "    x = upsample_block(in_layer, number_of_filters, filter_size, stride)\n",
    "    print('in shape: ', in_layer.shape, ' upsample with filter size ', filter_size, '; stride ', stride, ' -> out shape ', x.shape)\n",
    "    npt.assert_array_equal(x.shape, expected_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`down-sampling` class Encoder. Each block: 2 convolution layers with 3 filter_size and a non liniar ReLu. Blocks separated by max pooling with size of 2 and stride of 2.\n",
    "\n",
    "Parameters:\n",
    "`channel_numbers`: list of integers, the number of channels used for each encoder block.\n",
    "\n",
    "`retrun`: list of tensors, the output of each encoder block.\n",
    "\n",
    "Diagram used in tests:\n",
    "<img src=\"https://miro.medium.com/max/1400/1*J3t2b65ufsl1x6caf6GiBA.png\" style=\"width:40vw; margin-left:20vw\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in shape:  torch.Size([1, 64, 512, 512])  out shape  torch.Size([1, 64, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "class DoubleConvolution(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super(DoubleConvolution, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.conv(x)\n",
    "\n",
    "in_layer = torch.rand((1, 1, 512, 512)) # 1 number of images, 1 channel, 572x572 pixels\n",
    "expected_shape = (1, 64, 512, 512)\n",
    "model = DoubleConvolution(1, 64)\n",
    "in_layer = model(in_layer)\n",
    "print('in shape: ', in_layer.shape, ' out shape ', in_layer.shape)\n",
    "npt.assert_array_equal(in_layer.shape, expected_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in shape:  torch.Size([1, 1, 512, 512])  out shape  torch.Size([1, 512, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "class DownSamplingNeuralNetwork(nn.Module): # Encoder\n",
    "    def __init__(self, in_channels=1, features=[64,128,256,512]):\n",
    "        super(DownSamplingNeuralNetwork, self).__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConvolution(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skips.append(x)\n",
    "            x = self.pool(x)\n",
    "        return skips, x\n",
    "\n",
    "# Test the down sampling network\n",
    "in_layer = torch.rand((1, 1, 512, 512)) # 1 number of images, 1 channel, 572x572 pixels\n",
    "expected_shape = (1, 512, 64, 64)\n",
    "model = DownSamplingNeuralNetwork(in_channels=1,features=[64,128,256,512])\n",
    "skips, _ = model(in_layer)\n",
    "print('in shape: ', in_layer.shape, ' out shape ', skips[-1].shape)\n",
    "npt.assert_equal(skips[-1].shape, expected_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`up-sapling` class Decoder.\n",
    "\n",
    "Constructor parameters:`depth`: the depth of each decoder module\n",
    "\n",
    "Forward parameters:\n",
    "\n",
    "`x`: the input feature map\n",
    "`encoder_activations`: a list of activations from the encoder (for the skip connections)\n",
    "\n",
    "`Forward function`:\n",
    "\n",
    "1. up-sampling operation normalization and ReLU\n",
    "2. crop the activation map (use `CenterCrop`): to be the same size as the decoder block\n",
    "3. concatenate these two activation maps (on the channel dimension, 1)\n",
    "4. apply an encoder block on the result\n",
    "5. pass the result to the next decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in shape:  torch.Size([1, 512, 64, 64])  out shape  torch.Size([1, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "class UpSamplingNeuralNetwork(nn.Module):\n",
    "    def __init__(self, out_channels=1, features=[64,128,256,512]):\n",
    "        super(UpSamplingNeuralNetwork, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.ups.append(DoubleConvolution(feature*2, feature))\n",
    "        \n",
    "        self.bottleneck = DoubleConvolution(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x: Tensor, skip_connections) -> Tensor:\n",
    "        # x = self.pool(x)\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "        return self.final_conv(x)\n",
    "\n",
    "in_layer = torch.rand((1, 512, 64, 64))\n",
    "expected_shape = (1, 1, 512, 512)\n",
    "model = UpSamplingNeuralNetwork()\n",
    "output = model(in_layer, skips)\n",
    "print('in shape: ', in_layer.shape, ' out shape ', output.shape)\n",
    "npt.assert_equal(output.shape, expected_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 512, 512])\n",
      "torch.Size([1, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "class UNET_BY_UP_AND_DOWN(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, features=[64,128,256,512]):\n",
    "        super(UNET_BY_UP_AND_DOWN, self).__init__()\n",
    "        self.downs = DownSamplingNeuralNetwork(in_channels, features)\n",
    "        self.ups = UpSamplingNeuralNetwork(out_channels, features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skips, x = self.downs(x)\n",
    "        return self.ups(x, skips)\n",
    "\n",
    "in_layer = torch.rand((1, 1, 512, 512))\n",
    "model = UNET_BY_UP_AND_DOWN(in_channels=1, out_channels=1)\n",
    "output = model(in_layer)\n",
    "print(output.shape)\n",
    "print(in_layer.shape)\n",
    "assert output.shape == in_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`UNET` single class - simpler approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 512, 512])\n",
      "torch.Size([1, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, features=[64,128,256,512]):\n",
    "        super(UNET, self).__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConvolution(in_channels, feature))\n",
    "            in_channels = feature\n",
    "        \n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.ups.append(DoubleConvolution(feature*2, feature))\n",
    "        \n",
    "        self.bottleneck = DoubleConvolution(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        skip_connections = []\n",
    "        \n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        \n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "        \n",
    "        return self.final_conv(x)\n",
    "\n",
    "in_layer = torch.rand((1, 1, 512, 512))\n",
    "model = UNET(in_channels=1, out_channels=1)\n",
    "output = model(in_layer)\n",
    "print(output.shape)\n",
    "print(in_layer.shape)\n",
    "assert output.shape == in_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pre-trained` model: pre-trained weights on `ImageNet` and \"freeze these weights during the training process (set `required_grad=False` for those tensors).\n",
    "\n",
    "`Problem`: We neet to create the skip connections required by the U-Net architecture we need access to the feature maps of some intermediate layers in the network and these are not accessible by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
