{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hXhoiRUvnHJ"
      },
      "source": [
        "# Computer vision and deep learning - Laboratory 6\n",
        "\n",
        "In this last laboratory, we will switch our focus from implementing and training neural networks to developing a machine learning application.\n",
        "More specifically you will learn how you can convert your saved torch model into a more portable format using torch script and how you can create a simple demo application for your model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8spOviRwB3Uz"
      },
      "source": [
        "## Converting your model into portable TorchScript binaries\n",
        "\n",
        "\n",
        "``TorchScript`` allows you to create serializable and optimizable models from PyTorch code and then use them in a process where there is no Python dependency.\n",
        "\n",
        "\n",
        "When deploying our module in production systems, we might need to run the model using another programming language (not Python) and even on mobile or embedded devices. In addition, we need a more lightweight environment than the development one.\n",
        "\n",
        "\n",
        "Until now, when training a model we've saved checkpoints and reloaded the weights when needed into the development environment. As the name suggests, the checkpoints contain additional information (such as optimizer states) which allows you to resume the training process. However, all this information is not required during inference.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "``Torchscript`` allows you to create a lightweight and independent model artifact suitable for runtime via two different techniques: scripting and tracing. They are both used to convert a PyTorch model into a more optimized or deployable form.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Tracing involves capturing a model's execution trace by passing example inputs through the model and recording the operations executed. This creates a TorchScript representation of the model based on the traced operations. However, tracing might not capture all dynamic aspects of the model, especially if the model's behavior changes dynamically based on input data or control flow operations. Tracing is more focused on capturing the specific operations executed with example inputs, which might be more efficient but might not cover all dynamic behaviors of complex models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Scripting, on the other hand, refers to converting a PyTorch model (built using PyTorch's dynamic computation graph with Python control flow, such as loops and if statements) into a TorchScript. This involves representing the model as a static computation graph that can be executed independently of Python. Scripting allows the model to be saved and run in environments where a Python interpreter might not be available. Scripting captures the entire model logic and can handle more complex models with Pythonic control flow, making it more flexible but potentially more complex.\n",
        "\n",
        "\n",
        "Both techniques aim to transform PyTorch models into TorchScript representations, making them efficient for deployment in various environments or for optimized execution, albeit with different approaches. The choice between scripting and tracing depends on the specific use case, model complexity, and deployment requirements.\n",
        "\n",
        "You can check out the [documentation](https://pytorch.org/docs/stable/jit.html) for further details on ``TorchScript``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26fLZ04xnXyA"
      },
      "source": [
        "Below you have an example that demonstrates the conversion of a pre-trained ResNet-18 model from torchvision into a TorchScript and then loading and using the saved TorchScript model for inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import wget\n",
        "import glob\n",
        "import wandb\n",
        "import shutil\n",
        "import numpy as np\n",
        "import numpy.testing as npt\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import v2\n",
        "from torchvision.datasets import VisionDataset\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision.datasets.utils import check_integrity, download_and_extract_archive, download_url, verify_str_arg\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "import shutil\n",
        "from PIL import Image,ImageFile\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.testing as torch_testing\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import numpy.testing as npt\n",
        "from functools import reduce\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pickle\n",
        "import cv2\n",
        "from dataclasses import dataclass\n",
        "import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "import pprofile\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import torchmetrics\n",
        "import wandb\n",
        "import dataclasses\n",
        "import torchvision.models as models\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "P2gPwGpavl7C"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Create a sample input tensor (change according to your model's input requirements)\n",
        "example_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Script the model\n",
        "scripted_model = torch.jit.script(model)\n",
        "\n",
        "# Save the scripted model to a file\n",
        "scripted_model.save(\"scripted_resnet18.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DoubleConvolution(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        super(DoubleConvolution, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, features=[64,128,256,512]):\n",
        "        super(UNET, self).__init__()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # Down part of UNET\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConvolution(in_channels, feature))\n",
        "            in_channels = feature\n",
        "        \n",
        "        # Up part of UNET\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2)\n",
        "            )\n",
        "            self.ups.append(DoubleConvolution(feature*2, feature))\n",
        "        \n",
        "        self.bottleneck = DoubleConvolution(features[-1], features[-1]*2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "    \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        skip_connections = []\n",
        "        \n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "        \n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "        \n",
        "        for idx,up in enumerate(self.ups):\n",
        "            if (idx%2 == 0):\n",
        "                x=up(x)\n",
        "                if x.shape != skip_connections[idx//2].shape:\n",
        "                    x = TF.resize(x, size=skip_connections[idx//2].shape[2:])\n",
        "                x=torch.cat((skip_connections[idx//2],x),dim=1)\n",
        "            else:\n",
        "                x = up(x)\n",
        "        \n",
        "        return self.final_conv(x)\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class InputParameters:\n",
        "    model: nn.Module\n",
        "    optimizer: torch.optim.Optimizer\n",
        "    scheduler: torch.optim.lr_scheduler._LRScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Loading checkpoint\n"
          ]
        }
      ],
      "source": [
        "def load_model(filename):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    input_parameters = torch.load(filename)\n",
        "    return input_parameters.model\n",
        "\n",
        "# https://github.com/pytorch/pytorch/issues/47496\n",
        "segmentation_model = load_model(\"model.pth\")\n",
        "segmentation_model = torch.jit.script(segmentation_model)\n",
        "segmentation_model.save(\"segmentation_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Segmentation')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Example usage:\n",
        "transforms = v2.Compose([v2.Resize(128), v2.ToTensor()])\n",
        "with Image.open(\"example_1.jpg\") as image:\n",
        "    image.convert(\"RGB\")\n",
        "\n",
        "transformed_image = transforms(image).to(\"cuda\")\n",
        "transformed_image = transformed_image.unsqueeze(0)\n",
        "\n",
        "preds = segmentation_model(transformed_image).argmax(dim=1)\n",
        "computed_segmentation = F.one_hot(preds[0]).float().cpu().numpy()\n",
        "plt.imshow(computed_segmentation)\n",
        "plt.title(f\"Segmentation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBYTvmFYr9Fz"
      },
      "source": [
        "The main steps of the process are:\n",
        "- load the pre-trained model and set it to evaluation mode with model.eval().\n",
        "- create a sample input tensor (example_input) that matches the expected input shape of the model.\n",
        "- use ```torch.jit.script()``` to convert the model into a TorchScript representation.\n",
        "- save the scripted model to a file using ```scripted_model.save()``` for later use or deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eMTs3vbsV56"
      },
      "source": [
        "Now, let's see how you can use the scripted model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TO58HkOtsZXx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: tabby, Probability: 0.7649\n",
            "Prediction: tiger cat, Probability: 0.1408\n",
            "Prediction: Egyptian cat, Probability: 0.0876\n",
            "Prediction: Persian cat, Probability: 0.0024\n",
            "Prediction: lynx, Probability: 0.0013\n"
          ]
        }
      ],
      "source": [
        "from io import BytesIO\n",
        "import requests\n",
        "from torchvision.models import  ResNet18_Weights\n",
        "\n",
        "# Load the saved TorchScript model\n",
        "model = torch.jit.load(\"scripted_resnet18.pt\")\n",
        "\n",
        "\n",
        "preprocess = v2.Compose([\n",
        "    v2.Resize(256),\n",
        "    v2.CenterCrop(224),\n",
        "    v2.ToTensor(),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "image_url = 'https://images.unsplash.com/photo-1611267254323-4db7b39c732c?q=80&w=1000&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxzZWFyY2h8M3x8Y3V0ZSUyMGNhdHxlbnwwfHwwfHx8MA%3D%3D'\n",
        "response = requests.get(image_url)\n",
        "image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "input_tensor = preprocess(image)\n",
        "input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
        "with torch.no_grad():\n",
        "    # run the scripted model\n",
        "    output_image = model(input_batch)\n",
        "weights = ResNet18_Weights.DEFAULT\n",
        "class_names = weights.meta[\"categories\"]\n",
        "# Get the top 5 predictions\n",
        "probabilities = torch.nn.functional.softmax(output_image[0], dim=0)\n",
        "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "\n",
        "# Display top 5 predicted classes and their probabilities\n",
        "for i in range(top5_prob.size(0)):\n",
        "    class_idx = top5_catid[i].item()\n",
        "    print(f\"Prediction: {class_names[class_idx]}, Probability: {top5_prob[i].item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaEEfd1NOxyk"
      },
      "source": [
        "Optionally, you can also save the torchscript binary into ```wandb```. In this way, you will have a connection link between the model that is running in production and the training runs that you logged during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-btbzsULYXM"
      },
      "source": [
        "# Creating a simple UI with gradio\n",
        "\n",
        "\n",
        "[Gradio](https://www.gradio.app/docs/interface) is an open-source Python library used for creating customizable UI components for machine learning models with just a few lines of code. It greatly simplifies the process of building web-based interfaces to interact with ML models without requiring extensive knowledge of web development and allows you to quickly build an MVP and get feedback from the users.\n",
        "\n",
        "\n",
        "To get an application running, you just need to specify three parameters:\n",
        "1. the function to wrap the interface around.\n",
        "2. what are the desired input components?\n",
        "3. what are the desired output components?\n",
        "\n",
        "\n",
        "This is achieved through the ``gradio.Interface`` class, the central component in gradio, responsible for creating the user interface for your machine learning model.\n",
        "\n",
        "\n",
        "```\n",
        "import gradio as gr\n",
        "demo = gr.Interface(fn=image_classifier,\n",
        "                    inputs=\"image\",\n",
        "                    outputs=\"label\")\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Once you've defined the gr.Interface, the launch() method is used to start the interface, making it accessible through a web browser.\n",
        "\n",
        "\n",
        "```\n",
        "demo.launch()\n",
        "```\n",
        "\n",
        "\n",
        "When the launch method is called, ```gradio``` launches a simple web server that serves the demo. If you specify ```share=True``` when calling the launch function, ```gradio``` will create a public link Can also be used to create a public link used by anyone to access the demo from their browser.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXflaukmmWPh"
      },
      "source": [
        "## Simple UI for image classification in gradio\n",
        "\n",
        "Below you have an example of how you could use ```gradio``` to create a simple UI for an image classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "fpXMo37vMUYL",
        "outputId": "99a5b2a7-9173-42fc-96e7-ba2aba458039"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "preprocess = v2.Compose([\n",
        "    v2.Resize(256),\n",
        "    v2.CenterCrop(224),\n",
        "    v2.ToTensor(),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def preprocess_img(img):\n",
        "    img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
        "    img = preprocess(img)\n",
        "    return img\n",
        "\n",
        "def classify_image(img):\n",
        "    img = preprocess_img(img)\n",
        "    input_batch = img.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_batch)\n",
        "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "    confidences = {class_names[i]: float(probabilities[i]) for i in range(len(class_names))}\n",
        "    return confidences\n",
        "\n",
        "ui = gr.Interface(fn=classify_image,\n",
        "             inputs=gr.Image(),\n",
        "             outputs=gr.Label(num_top_classes=3),\n",
        "             examples=['Abyssinian.jpg', 'Bengal.jpg'],\n",
        "          )\n",
        "ui.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKXtFC0_hkcm"
      },
      "source": [
        "## Accessing the webcam with gradio\n",
        "\n",
        "In the example below, you have an example in which you take the input images from your webcam.\n",
        "The function wrapped by gradio uses a mask to blur the input image outside that mask. If you plan to do background blurring, the mask could be the segmentation mask predicted by your model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "HzC5frqLhk1I",
        "outputId": "c1e40a6a-c49b-4a93-f914-fce4eeb17341"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7868\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "transforms = v2.Compose([v2.Resize(128), v2.ToTensor()])\n",
        "\n",
        "def get_segmentation(input_image):\n",
        "    if isinstance(input_image, str):\n",
        "        input_image = np.array(Image.open(input_image).convert(\"RGB\"))\n",
        "    else:\n",
        "        input_image = Image.fromarray(input_image.astype('uint8'), 'RGB').convert(\"RGB\")\n",
        "\n",
        "    transformed_image = transforms(input_image).to(\"cuda\")\n",
        "    transformed_image = transformed_image.unsqueeze(0)\n",
        "\n",
        "    preds = segmentation_model(transformed_image).argmax(dim=1)\n",
        "    computed_segmentation = F.one_hot(preds[0]).float().cpu().numpy()\n",
        "    # Convert to gradio:\n",
        "    return computed_segmentation\n",
        "\n",
        "\n",
        "ui = gr.Interface(\n",
        "    fn=get_segmentation,\n",
        "    live=True,\n",
        "    inputs=[gr.Image(sources=[\"webcam\"], streaming=True)],\n",
        "    outputs=[gr.Image()],\n",
        "    title=\"Image segmentation demo!\",\n",
        ")\n",
        "ui.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhaGtNlJj1Wr"
      },
      "source": [
        "## Laboratory assignment\n",
        "\n",
        "\n",
        "Now you have all the knowledge required to build your own ML semantic segmentation application.\n",
        "\n",
        "\n",
        "1. First use ```torchscript``` to obtain a model binary.\n",
        "2. Using gradio, create a simple application that uses the semantic segmentation that you developed. Feel free to define the scope and the functional requirements of your app.\n",
        "3. __[Optional, independent work]__ Use a serverless cloud function on [AWS Lambda](https://aws.amazon.com/lambda/) (this requires an account on Amazon AWS and you need to provide the details of a credit card) to run the prediction and get the results.\n",
        "\n",
        "\n",
        "Congratulations, you've just completed all the practical work for Computer Vision and Deep Learning!\n",
        "May your data always be clean, your models accurate, and your code bug-free!\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
